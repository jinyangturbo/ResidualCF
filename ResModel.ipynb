{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doublewrap(function):\n",
    "    \"\"\"\n",
    "    A decorator decorator, allowing to use the decorator to be used without\n",
    "    parentheses if not arguments are provided. All arguments must be optional.\n",
    "    \"\"\"\n",
    "\n",
    "    @functools.wraps(function)\n",
    "    def decorator(*args, **kwargs):\n",
    "        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n",
    "            return function(args[0])\n",
    "        else:\n",
    "            return lambda wrapee: function(wrapee, *args, **kwargs)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@doublewrap\n",
    "def define_scope(function, scope=None, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    A decorator for functions that define TensorFlow operations. The wrapped\n",
    "    function will only be executed once. Subsequent calls to it will directly\n",
    "    return the result so that operations are added to the graph only once.\n",
    "    The operations added by the function live within a tf.variable_scope(). If\n",
    "    this decorator is used with arguments, they will be forwarded to the\n",
    "    variable scope. The scope name defaults to the name of the wrapped\n",
    "    function.\n",
    "    \"\"\"\n",
    "    attribute = '_cache_' + function.__name__\n",
    "    name = scope or function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            with tf.variable_scope(name, *args, **kwargs):\n",
    "                setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def block(inputs):\n",
    "    short_cut = inputs\n",
    "    inputs = tf.contrib.slim.fully_connected(inputs, size)\n",
    "    inputs = tf.contrib.slim.fully_connected(inputs, size)\n",
    "    inputs = tf.contrib.slim.fully_connected(inputs, short_cut.shape[1])\n",
    "    return inputs+short_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, input_user, input_item, output, num_users, num_items):\n",
    "        self.input_user = input_user\n",
    "        self.input_item = input_item\n",
    "        self.output = output\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.predict\n",
    "        self.optimize\n",
    "    \n",
    "    @define_scope(initializer=tf.contrib.slim.xavier_initializer())\n",
    "    def predict(self):\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            embedding_users = tf.get_variable(\"embedding_users\", [self.num_users, self.embedding_size])\n",
    "            embedding_items = tf.get_variable(\"embedding_items\", [self.num_items, self.embedding_size])\n",
    "            self.embedding_users = embedding_users\n",
    "            self.embedding_items = embedding_items\n",
    "            user_embedding = tf.reduce_sum(tf.nn.embedding_lookup(embedding_users, self.input_user), axis=1)\n",
    "            item_embedding = tf.reduce_sum(tf.nn.embedding_lookup(embedding_items, self.input_item), axis=1)\n",
    "            merge_embedding = tf.concat([user_embedding, item_embedding], axis=1, name=\"merge_embedding\")\n",
    "            tf.summary.histogram(\"embedding_users\",embedding_users)\n",
    "            tf.summary.histogram(\"embedding_items\",embedding_items)\n",
    "\n",
    "#             embedding_users_g = tf.get_variable(\"embedding_users_GMF\", [self.num_users, self.embedding_size])\n",
    "#             embedding_items_g = tf.get_variable(\"embedding_items_GMF\", [self.num_items, self.embedding_size])\n",
    "#             user_embedding_g = tf.reduce_sum(tf.nn.embedding_lookup(embedding_users_g, self.input_user), axis=1)\n",
    "#             item_embedding_g = tf.reduce_sum(tf.nn.embedding_lookup(embedding_items_g, self.input_item), axis=1)\n",
    "#             GMF_embed = user_embedding_g*item_embedding_g\n",
    "#         x = merge_embedding\n",
    "#         with tf.name_scope(\"fc\"):\n",
    "#             for i in xrange(len(self.layers) - 1):\n",
    "#                 x = tf.contrib.slim.fully_connected(x, self.layers[i + 1])\n",
    "#             if not self.MLP:\n",
    "#                 x = tf.concat([x, GMF_embed], axis=1, name = \"concat_embedding\")\n",
    "#                 print (\"MLP is False, Running NeuMF\")\n",
    "#             x = tf.contrib.slim.fully_connected(x, 1, tf.identity)\n",
    "        x = block(x)\n",
    "        x = block(x)\n",
    "        x = block(x)\n",
    "        x = tf.contrib.slim.fully_connected(x, 1, tf.identity)\n",
    "        return x\n",
    "    \n",
    "    @define_scope\n",
    "    def optimize(self):\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predict,\n",
    "                                                           labels=self.output,name=\"cross_entropy\")\n",
    "            # tf.summary.scalar(\"cross_entropy\", loss)\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "        return optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
